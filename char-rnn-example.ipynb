{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char-RNN in Pytorch: The Wisdom of Marx\n",
    "\n",
    "Let's try to implement to implement [Andrej's minmal char-RNN](https://gist.github.com/karpathy/d4dee566867f8291f086) in Pytorch! The difference is that we'll use LSTM layers instead of vanilla RNN, and we'll do it in batches with GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus has 1468303 letters altogether\n",
      "corpus has 108 unique characters: ['\\n', ' ', '!', '\"', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}', '\\x91', '\\x92', '\\x93', '\\x94', '\\x96', '\\xa0', '£', '°', '¼', '½', '¾', '×', 'à', 'â', 'æ', 'è', 'é', 'ê', 'î', 'ï', 'ô', 'û', 'ü']\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "cudafloat = torch.cuda.FloatTensor \n",
    "cudalong = torch.cuda.LongTensor\n",
    "\n",
    "raw_text = open('capital-vol1.txt', encoding='latin-1', mode='r').read()\n",
    "chars = sorted(set(raw_text))\n",
    "print('corpus has ' + str(len(raw_text)) + ' letters altogether')\n",
    "print ('corpus has ' + str(len(chars)) + ' unique characters:', chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll clean up the text so that our output is limited to lower cased english characters plus simple punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus has 1427555 letters altogether\n",
      "corpus has 32 unique characters after cleaning: {'c', 'd', 's', 'r', '-', 'u', 'q', 'f', 'e', 'v', 'w', '.', 'g', 'x', 'a', 'o', \"'\", 'k', 'y', 'i', 'h', 'm', 'n', 'z', 'p', 't', ' ', ',', 'b', 'l', 'j', ';'}\n"
     ]
    }
   ],
   "source": [
    "# define the set of letters we want (ascii)\n",
    "all_letters = string.ascii_lowercase + \" .,;'-\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# function to clean raw text\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "text = unicodeToAscii(raw_text)\n",
    "text_length = len(text)\n",
    "print('corpus has ' + str(text_length) + ' letters altogether')\n",
    "print ('corpus has ' + str(len(set(text))) + ' unique characters after cleaning:', set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setup idx and char mapping\n",
    "chars_to_idx = dict((c, i) for i, c in enumerate(all_letters))\n",
    "idx_to_chars = dict((i, c) for i, c in enumerate(all_letters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting text to vectors\n",
    "\n",
    "We begin by creating our list of inputs and outputs. The step size indicates how many characters we move ahead for every sample we create. We generate the biggest sample size with the step size of one, and also the most redundancy in samples (which is not necessarily bad for our task).\n",
    "1. inputs - list of win_size strings\n",
    "2. outputs - list of character following the input string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['hello', 'ello ', 'llo w', 'lo wo', 'o wor', ' worl'], [' ', 'w', 'o', 'r', 'l', 'd'])\n"
     ]
    }
   ],
   "source": [
    "def textToWin(text, win_size, step_size):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for i in range(0, len(text) - win_size, step_size):\n",
    "        window = text[i:win_size+i]\n",
    "        inputs.append(window)\n",
    "    outputs = [i for i in text[win_size::step_size]]\n",
    "    return inputs, outputs\n",
    "\n",
    "print(textToWin('hello world', 5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we the previous function to help convert our text list into torch tensors of indices. It's important to note that for tensors, the dimensions are explicit and viewable, whereas for a list, you can only view the most shallow dimension without iterating through it. \n",
    "\n",
    "You NEED to put your input into the right tensor format.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    7     4    11    11    14\n",
      "   11    11    14    26    13\n",
      "   14    26    13     8     2\n",
      "   13     8     2     4    26\n",
      "    2     4    26    19    14\n",
      "   26    19    14    26    12\n",
      "   14    26    12     4     4\n",
      "   12     4     4    19    26\n",
      "    4    19    26    24    14\n",
      "[torch.LongTensor of size 9x5]\n",
      " \n",
      " 26\n",
      "  8\n",
      "  4\n",
      " 19\n",
      " 26\n",
      "  4\n",
      " 19\n",
      " 24\n",
      " 20\n",
      "[torch.LongTensor of size 9]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def textToTensor(text, win_size, step_size):\n",
    "    inputs, outputs = textToWin(text, win_size, step_size)\n",
    "    \n",
    "    X = torch.zeros(len(inputs), win_size).long()\n",
    "    y = torch.zeros(len(inputs)).long()\n",
    "    \n",
    "    for i, sent in enumerate(inputs):\n",
    "        for t, char in enumerate(sent):\n",
    "            X[i, t] = chars_to_idx[sent[t]]\n",
    "        y[i] = chars_to_idx[outputs[i]]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "test_text = \"hello nice to meet you\"\n",
    "\n",
    "testX, testy = textToTensor(test_text, 5, 2)\n",
    "print(testX, testy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the model\n",
    "import time\n",
    "\n",
    "class LSTMText(nn.Module):\n",
    "    def __init__(self, window_size, hidden_size, n_layers, batch_size, dropout, num_embed=n_letters):\n",
    "        super(LSTMText, self).__init__()\n",
    "        self.num_embed = num_embed\n",
    "        self.embed_dim = hidden_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.window_size = window_size\n",
    "        self.output_size = num_embed\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.encoder = nn.Embedding(num_embed, self.embed_dim)\n",
    "        self.lstm = nn.LSTM(self.embed_dim, hidden_size, n_layers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(hidden_size, self.output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.logsoftmax = nn.LogSoftmax()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        h0 = Variable(torch.zeros(self.n_layers, self.batch_size, self.hidden_size).type(cudafloat))\n",
    "        c0 = Variable(torch.zeros(self.n_layers, self.batch_size, self.hidden_size).type(cudafloat))\n",
    "        return h0, c0\n",
    "    \n",
    "    def forward(self, inputs, hidden):                  # takes input tensor of (batch_size, # of indices to extract = win_size)\n",
    "        embed = self.encoder(inputs)                    # outputs (batch_size, seq_len, embedding_dim)\n",
    "        embed = embed.view(batch_size, window_size, hidden_size)  # maintains 3D when batch size of 1 is passed into model\n",
    "        embed = embed.permute(1, 0, 2)                  # getting dimensions right for LSTM.. DON\"T use tensor.view\n",
    "        output, hidden = self.lstm(embed, hidden)       # input tensor of (seq_len, batch, input_size), output (seq, batch, input_size)\n",
    "        output = output[window_size-1, :, :]            # select the last vector in the seq_len (the last character)\n",
    "        decoded = self.decoder(output)                  # decoded output = (batch, output)\n",
    "        pred = self.logsoftmax(decoded)\n",
    "        \n",
    "        return pred, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "window_size = 100\n",
    "output_size = n_letters\n",
    "hidden_size = 500\n",
    "batch_size = 512\n",
    "n_layers = 2\n",
    "dropout = 0.3\n",
    "\n",
    "model = LSTMText(window_size=window_size, hidden_size=hidden_size, n_layers=n_layers, batch_size=batch_size, dropout=dropout)\n",
    "model.cuda()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss(size_average=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 1.468 tijme: 177.984\n",
      "[2] loss: 1.216 tijme: 175.426\n",
      "[3] loss: 1.143 tijme: 177.457\n",
      "[4] loss: 1.095 tijme: 177.383\n",
      "[5] loss: 1.059 tijme: 177.394\n",
      "[6] loss: 1.030 tijme: 177.512\n",
      "[7] loss: 1.006 tijme: 177.998\n",
      "[8] loss: 0.983 tijme: 178.302\n",
      "[9] loss: 0.964 tijme: 178.291\n",
      "[10] loss: 0.946 tijme: 178.349\n",
      "[11] loss: 0.929 tijme: 178.312\n",
      "[12] loss: 0.915 tijme: 178.395\n",
      "[13] loss: 0.902 tijme: 178.246\n",
      "[14] loss: 0.888 tijme: 178.348\n",
      "[15] loss: 0.877 tijme: 178.272\n",
      "[16] loss: 0.866 tijme: 178.354\n",
      "[17] loss: 0.856 tijme: 178.352\n",
      "[18] loss: 0.847 tijme: 178.357\n",
      "[19] loss: 0.840 tijme: 178.410\n",
      "[20] loss: 0.831 tijme: 178.370\n",
      "[21] loss: 0.823 tijme: 178.362\n",
      "[22] loss: 0.818 tijme: 178.340\n",
      "[23] loss: 0.812 tijme: 178.254\n",
      "[24] loss: 0.806 tijme: 178.376\n",
      "[25] loss: 0.802 tijme: 178.299\n",
      "[26] loss: 0.797 tijme: 178.419\n",
      "[27] loss: 0.792 tijme: 178.348\n",
      "[28] loss: 0.790 tijme: 178.353\n",
      "[29] loss: 0.786 tijme: 178.442\n",
      "[30] loss: 0.782 tijme: 178.341\n",
      "[31] loss: 0.779 tijme: 178.438\n",
      "[32] loss: 0.776 tijme: 178.423\n",
      "[33] loss: 0.774 tijme: 178.395\n",
      "[34] loss: 0.771 tijme: 178.440\n",
      "[35] loss: 0.771 tijme: 178.517\n",
      "[36] loss: 0.770 tijme: 178.501\n",
      "[37] loss: 0.766 tijme: 178.553\n",
      "[38] loss: 0.764 tijme: 178.444\n",
      "[39] loss: 0.763 tijme: 178.480\n",
      "[40] loss: 0.763 tijme: 178.411\n"
     ]
    }
   ],
   "source": [
    "step_size=3\n",
    "n_epochs = 40\n",
    "losses = []\n",
    "\n",
    "X, y = textToTensor(text, window_size, step_size)\n",
    "dataset = torch.utils.data.TensorDataset(X, y)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=1, pin_memory=True, drop_last=True)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    hidden = model.init_hidden()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # send input to GPU and wrap in torch Variable\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "             \n",
    "        # init, forward, backward, optimize\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data[0]/len(train_loader)\n",
    "        \n",
    "    print('[%d] loss: %.3f tijme: %.3f' % (epoch + 1, running_loss, time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'pytorch_test_weight6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating text\n",
    "We create a new model with a batch_size of 1 to take in one window of text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "model_test = LSTMText(window_size=window_size, hidden_size=hidden_size, n_layers=n_layers, batch_size=batch_size, dropout=dropout)\n",
    "model_test.cuda()\n",
    "model_test.eval()\n",
    "model_test.load_state_dict(torch.load('pytorch_test_weight6'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "  7\n",
      "  4\n",
      " 11\n",
      " 11\n",
      " 14\n",
      " 26\n",
      "  8\n",
      " 26\n",
      "  0\n",
      " 12\n",
      " 26\n",
      " 12\n",
      " 17\n",
      "[torch.cuda.LongTensor of size 13 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "    7     4    11    11    14    26     8    26     0    12    26    12    17\n",
      "[torch.cuda.LongTensor of size 1x13 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# used in generation\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long().cuda()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = chars_to_idx[string[c]]\n",
    "    return Variable(tensor)\n",
    "\n",
    "a = char_tensor('hello i am mr')\n",
    "print (a)\n",
    "print (a.view(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"nd. er technical means for saving labour are colossal. evertheless, if to-morrow morning labour gene\"\n",
      "nd. er technical means for saving labour are colossal. evertheless, if to-morrow morning labour generally, the capitalist system and transformation of the commodities have been converted into commodity being less than half of the producer. he superficial elements of the population is at the same time that the producer is the producer of the poor, who had been expropriated, the tate of the poor case not only the movement of the wage labourer is determined by the labourer as the conditions of the \n",
      "\n",
      "----- temperature: 0.8\n",
      "----- Generating with seed: \"nd. er technical means for saving labour are colossal. evertheless, if to-morrow morning labour gene\"\n",
      "nd. er technical means for saving labour are colossal. evertheless, if to-morrow morning labour generally, and one of the old systematical economists of the modern medium was produced is the total capital. n this point, the advance of a case, it is capitalist apply to every yarn, etculo used the stubjest of agricultural labour, causes the labourer himself. onsequence wages, the successful capitalist began. nstead of the farmers are thencefore private property, not into capital, no matter we lebt\n",
      "\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"nd. er technical means for saving labour are colossal. evertheless, if to-morrow morning labour gene\"\n",
      "nd. er technical means for saving labour are colossal. evertheless, if to-morrow morning labour generally, for the direct proportion of the world.  capitalist production comprehension over the repeading of every , into particular process. f the clan was the absolute muscless of the mines, always r. minuor, contracting from his bundstant revolutionics and wage-labourers was.... he fundamental form of labour generally, of vaguants transformstate of this capital. t is only the demore of production \n"
     ]
    }
   ],
   "source": [
    "def pred_text(pred_len):\n",
    "    start_index = random.randint(0, len(text) - window_size - 1)\n",
    "    hidden = model_test.init_hidden()\n",
    "    \n",
    "    for temperature in [0.5, 0.8, 1.0]:\n",
    "        print()\n",
    "        print('----- temperature:', temperature)\n",
    "\n",
    "        textX = text[start_index: start_index + window_size]\n",
    "        print('----- Generating with seed: \"' + textX + '\"')\n",
    "        sys.stdout.write(textX)\n",
    "        inp = char_tensor(textX)\n",
    "        inp = torch.unsqueeze(inp, 0)\n",
    "        \n",
    "        for i in range(pred_len):\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            output, hidden = model_test(inp, hidden)\n",
    "            output_dist = output.view(-1).div(temperature).exp().cpu()\n",
    "            top_i = torch.multinomial(output_dist, 1)[0]\n",
    "            top_num = top_i.data[0]\n",
    "            pred_char = idx_to_chars[top_num]\n",
    "            textX += pred_char\n",
    "            textX = textX[1:]\n",
    "            inp = char_tensor(textX)\n",
    "            sys.stdout.write(pred_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "        \n",
    "pred_text(400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
