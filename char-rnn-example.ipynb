{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char-RNN in Pytorch: The Wisdom of Marx\n",
    "\n",
    "Let's try to implement to implement [Andrej's minmal char-RNN](https://gist.github.com/karpathy/d4dee566867f8291f086) to generate text in Pytorch! The difference is that we'll use LSTM layers instead of vanilla RNN, and we'll do it in batches with GPU. \n",
    "\n",
    "Compared to the [Intermediate RNN tutorials on Pytorch's website](http://pytorch.org/tutorials/), the main difference is that this tutorial will take more advantage of the GPU, by taking in multiple words/characters at a time, and in batches.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "First we acquire the MS word version of [Das Kapital Volume 1](https://www.marxists.org/archive/marx/works/1867-c1/) and turned it into a text format that we can load. Let's see a snippet of it as well.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1: Commodities\\nSection 1: The Two Factors of a Commodity:\\nUse-Value and Value\\n(The Substance of Value and the Magnitude of Value)\\nThe wealth of those societies in which the capitalist mode of production prevails, presents itself as \\x93an immense accumulation of commodities,\\x941 its unit being a single commodity. Our investigation must therefore begin with the analysis of a commodity. \\nA commodity is, in the first place, an object outside us, a thing that by its properties satisfies human wants of some sort or another. The nature of such wants, whether, for instance, they spring from the stomach or from fancy, makes no difference.2 Neither are we here concerned to know how the object satisfies these wants, whether directly as means of subsistence, or indirectly as means of production. \\nEvery useful thing, as iron, paper, &c., may be looked at from the two points of view of quality and quantity. It is an assemblage of many properties, and may therefore be of use in various ways. To d'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text = open('capital-vol1.txt', encoding='latin-1', mode='r').read()\n",
    "raw_text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! Let's print out all the characters in the text to see what we will be getting rid of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus has 1468303 letters altogether\n",
      "corpus has 108 unique characters: ['\\n', ' ', '!', '\"', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}', '\\x91', '\\x92', '\\x93', '\\x94', '\\x96', '\\xa0', '£', '°', '¼', '½', '¾', '×', 'à', 'â', 'æ', 'è', 'é', 'ê', 'î', 'ï', 'ô', 'û', 'ü']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(raw_text))\n",
    "print('corpus has ' + str(len(raw_text)) + ' letters altogether')\n",
    "print ('corpus has ' + str(len(chars)) + ' unique characters:', chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll clean up the text so that our output is limited to lower cased english characters plus simple punctuations. The cleaning function is taken from [stackoverflow](#https://stackoverflow.com/questions/517923/what-is-the-best-way-to-remove-accents-in-a-python-unicode-string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus has 1427555 letters altogether\n",
      "corpus has 32 unique characters after cleaning: {';', 'y', 'i', 'o', 's', 'g', 'd', 'z', 't', 'b', 'a', 'k', 'r', 'p', 'c', 'u', 'w', '-', 'f', 'n', 'q', 'm', 'e', 'l', 'v', ',', 'j', \"'\", '.', 'h', 'x', ' '}\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_lowercase + \" .,;'-\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "text = unicodeToAscii(raw_text)\n",
    "text_length = len(text)\n",
    "print('corpus has ' + str(text_length) + ' letters altogether')\n",
    "print ('corpus has ' + str(len(set(text))) + ' unique characters after cleaning:', set(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, though we can probably expect certain words with non-english characters to be misspelled. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting text to vectors\n",
    "\n",
    "To create out dataset, we begin by creating a list of inputs and outputs. Our input should be a string of text, say 100 characters long (we will call this sequence length), and the output should be the next character following it. We can decide how many samples we want and how redundant they should be with the step size. The step size indicates how many characters we move ahead for every sample we create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample size =  6 ['hello', 'ello ', 'llo w', 'lo wo', 'o wor', ' worl']\n",
      "sample size =  6 [' ', 'w', 'o', 'r', 'l', 'd']\n"
     ]
    }
   ],
   "source": [
    "def textToWin(text, seq_len, step_size):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for i in range(0, len(text) - seq_len, step_size):\n",
    "        window = text[i:seq_len+i]\n",
    "        inputs.append(window)\n",
    "    outputs = [i for i in text[seq_len::step_size]]\n",
    "    return inputs, outputs\n",
    "\n",
    "inptest, outtest = textToWin('hello world', 5, 1)\n",
    "print(\"sample size = \", len(inptest), inptest)\n",
    "print(\"sample size = \", len(outtest), outtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we incorporate the previous function and turn the outputted list of strings into tensors of indices (aka vectors), so that our model can process them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setup idx and char mapping \n",
    "chars_to_idx = dict((c, i) for i, c in enumerate(all_letters))\n",
    "idx_to_chars = dict((i, c) for i, c in enumerate(all_letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  7   4  11  11  14\n",
      " 11  11  14  26  22\n",
      " 14  26  22  14  17\n",
      "[torch.LongTensor of size 3x5]\n",
      " \n",
      " 26\n",
      " 14\n",
      " 11\n",
      "[torch.LongTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def textToTensor(text, seq_len, step_size):\n",
    "    inputs, outputs = textToWin(text, seq_len, step_size)\n",
    "    X = torch.zeros(len(inputs), seq_len).long()\n",
    "    y = torch.zeros(len(inputs)).long()\n",
    "    for i, seq in enumerate(inputs):\n",
    "        for t, char in enumerate(seq):\n",
    "            X[i, t] = chars_to_idx[seq[t]]\n",
    "        y[i] = chars_to_idx[outputs[i]]\n",
    "    # outputs X, y - (sample_size, seq_len), (sample_size) with value 0 < c < n_letters\n",
    "    return X, y\n",
    "\n",
    "test_text = \"hello world\"\n",
    "testX, testy = textToTensor(test_text, 5, 2)\n",
    "print(testX, testy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the previous function to get the total sample size and split them into training set and valuation set. We roll the data set out in mini-batches by packing them into batch-sized generators.  We enumerate through the packed geneator to get X of (batch_size, seq_len) shape and y of (batch_size). Realize that taking the total sample size of the unpacked X and dividing that by the batch size gives us about the same number as the length of the packed X and y combine. It is slightly less because we tell it to drop the last batch if its size differs from the rest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "929.333984375\n",
      "928\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data\n",
    "N = 512\n",
    "W = 100\n",
    "step_size=3\n",
    "\n",
    "X, y = textToTensor(text, W, step_size)\n",
    "sample_size = len(y)\n",
    "train_size = int(0.9*sample_size)\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:], y[train_size:]\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=N, num_workers=1, pin_memory=True, drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=N, num_workers=1, pin_memory=True, drop_last=True)\n",
    "\n",
    "print (len(X)/N)\n",
    "print(len(train_loader) + len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time\n",
    "\n",
    "# we will need to specifiy the variables as cuda to utilize GPU\n",
    "cudafloat = torch.cuda.FloatTensor \n",
    "cudalong = torch.cuda.LongTensor\n",
    "\n",
    "'''\n",
    "W, H, N = seq_length, hidden_size, batch_size\n",
    "'''\n",
    "\n",
    "class LSTMText(nn.Module):\n",
    "    def __init__(self, W, H, n_layers, N, dropout, num_embed=n_letters):\n",
    "        super(LSTMText, self).__init__()\n",
    "        self.num_embed = num_embed\n",
    "        self.embed_dim = H\n",
    "        self.H = H\n",
    "        self.n_layers = n_layers\n",
    "        self.W = W\n",
    "        self.out_dim = num_embed\n",
    "        self.N = N\n",
    "        \n",
    "        self.encoder = nn.Embedding(num_embed, self.embed_dim)\n",
    "        self.lstm = nn.LSTM(self.embed_dim, H, n_layers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(H, self.out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.logsoftmax = nn.LogSoftmax()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        h0 = Variable(torch.zeros(n_layers, N, H).type(cudafloat))\n",
    "        c0 = Variable(torch.zeros(n_layers, N, H).type(cudafloat))\n",
    "        return h0, c0\n",
    "    \n",
    "    def forward(self, inputs, hidden):                  \n",
    "        embed = self.encoder(inputs)     # (N, W) => (N, W, embed_dim) \n",
    "        embed = embed.view(N, W, H)      # maintains 3D when N=1\n",
    "        embed = embed.permute(1, 0, 2)   # (N, W, embed_dim) => (W, N, embed_dim)\n",
    "        output, hidden = self.lstm(embed, hidden)   # (W, N, embed_dim) => (W, N, H)\n",
    "        output = output[W-1, :, :]        # select  last vector of W (last character)\n",
    "        decoded = self.decoder(output)              # (N, H) => (N, out_dim)\n",
    "        pred = self.logsoftmax(decoded)\n",
    "        \n",
    "        return pred, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function disconnect the hidden cells from its history. We need this because we do not ned to backpropagate through the entire history of updates to the hidden cells. \n",
    "[source](https://discuss.pytorch.org/t/help-clarifying-repackage-hidden-in-word-language-model/226/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    \"\"\"source: https://github.com/pytorch/examples/tree/master/word_language_model\"\"\"\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we instantiate the model and send it to the GPU with model.cuda(), and specify our update method and loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "W = W\n",
    "H = 500\n",
    "n_layers = 2\n",
    "dropout = 0.3\n",
    "\n",
    "model = LSTMText(W=W, H=H, n_layers=n_layers, N=N, dropout=dropout)\n",
    "model.cuda()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss(size_average=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] train loss: 1.476 val loss: 1.460 time: 163.152\n",
      "saving least val loss model from epoch [1]\n",
      "[2] train loss: 1.212 val loss: 1.389 time: 165.259\n",
      "saving least val loss model from epoch [2]\n",
      "[3] train loss: 1.136 val loss: 1.351 time: 166.735\n",
      "saving least val loss model from epoch [3]\n",
      "[4] train loss: 1.086 val loss: 1.336 time: 167.744\n",
      "saving least val loss model from epoch [4]\n",
      "[5] train loss: 1.049 val loss: 1.328 time: 167.248\n",
      "saving least val loss model from epoch [5]\n",
      "[6] train loss: 1.017 val loss: 1.324 time: 167.107\n",
      "saving least val loss model from epoch [6]\n",
      "[7] train loss: 0.991 val loss: 1.324 time: 167.117\n",
      "saving least val loss model from epoch [7]\n",
      "[8] train loss: 0.967 val loss: 1.328 time: 167.026\n",
      "[9] train loss: 0.946 val loss: 1.335 time: 166.811\n",
      "[10] train loss: 0.928 val loss: 1.344 time: 167.257\n",
      "[11] train loss: 0.911 val loss: 1.356 time: 166.953\n",
      "[12] train loss: 0.894 val loss: 1.357 time: 167.223\n",
      "[13] train loss: 0.879 val loss: 1.369 time: 167.178\n",
      "[14] train loss: 0.866 val loss: 1.381 time: 167.032\n",
      "[15] train loss: 0.854 val loss: 1.391 time: 165.117\n",
      "[16] train loss: 0.842 val loss: 1.404 time: 164.614\n",
      "[17] train loss: 0.831 val loss: 1.407 time: 163.709\n",
      "[18] train loss: 0.822 val loss: 1.420 time: 163.526\n",
      "[19] train loss: 0.812 val loss: 1.429 time: 163.468\n",
      "[20] train loss: 0.803 val loss: 1.440 time: 163.551\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "graph_train, graph_val = [], []\n",
    "best_val_loss = 100.0\n",
    "n_epochs = 20\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = 0.0, 0.0\n",
    "    val_loss, val_acc = 0.0, 0.0\n",
    "    hidden = model.init_hidden()\n",
    "    \n",
    "    # train\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # send input to GPU and wrap in torch Variable\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "             \n",
    "        # forward, backward, optimize\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.data[0]/len(train_loader)\n",
    "    \n",
    "    \n",
    "    # evaluate with validation set\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(val_loader):\n",
    "        # send input to GPU and wrap in torch Variable\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "             \n",
    "        # forward, backward, optimize\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, target)\n",
    "       \n",
    "        val_loss += loss.data[0]/len(val_loader)\n",
    "    \n",
    "    graph_train.append(train_loss)\n",
    "    graph_val.append(val_loss)\n",
    "    \n",
    "    print('[%d] train loss: %.3f val loss: %.3f time: %.3f' % (epoch + 1, train_loss, val_loss, time.time() - start_time))\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_loss_weight') \n",
    "        print('saving least val loss model from epoch [%d]'% (epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njDH1LCkYY4ypZ0nBGGNMPUsKxhhj6v3/H1bfgsnrmC4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a7c18fe518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.ylabel('Avg. loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.plot(graph_train)\n",
    "plt.plot(graph_val)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'last_weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating text\n",
    "We create a new model with a batch_size of 1 to take in one sequence of text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "N = 1\n",
    "\n",
    "model_test = LSTMText(W=W, H=H, n_layers=n_layers, N=N, dropout=dropout)\n",
    "model_test.cuda()\n",
    "model_test.eval()\n",
    "model_test.load_state_dict(torch.load('last_weight'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "  7\n",
      "  4\n",
      " 11\n",
      " 11\n",
      " 14\n",
      "[torch.cuda.LongTensor of size 5 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long().cuda()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = chars_to_idx[string[c]]\n",
    "    return Variable(tensor)\n",
    "\n",
    "a = char_tensor('hello')\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"o its inmates. he exchange becomes an accomplished fact by two metamorphoses of opposite yet supplem\"\n",
      "ent. e were received that the compinitation of the most part weekly wages that the conditions of the industrial combinations of the workhouse had done fit for instance, and in the history of wages, and the dead with surplus population, that the former decreasing the present degree of the individual capitals suddenly means of production have been accumulated. he colliers will be invosible to the ab\n",
      "\n",
      "----- temperature: 0.8\n",
      "----- Generating with seed: \"o its inmates. he exchange becomes an accomplished fact by two metamorphoses of opposite yet supplem\"\n",
      "ent to the soil as if the same time a million to the private labourers workshops, and for instance, but in the whole great part of the poorer line. uch additional manufacturers, man and partiarishes the direct and moral less character. o the parish, nay growned up the farmer, may have made him during the expenses of the shorter few during the children than there to the agricultural producers, depe\n",
      "\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"o its inmates. he exchange becomes an accomplished fact by two metamorphoses of opposite yet supplem\"\n",
      "ents. hey did nearly re-trainsances from, there a struggles gave this life, others, and increased, and this, the parish, and with the bentlember, inhabitantens insufficiency necessary to true than  turn is once people is dwilling; on the worst, taken from the ginane means of subsistence and a larger principels. n the one hand, the expenses of equivalence, takes the viltage uncommon by the use of h\n"
     ]
    }
   ],
   "source": [
    "def pred_text(pred_len):\n",
    "    start_index = random.randint(0, len(text) - W - 1)\n",
    "    hidden = model_test.init_hidden()\n",
    "    \n",
    "    for temperature in [0.5, 0.8, 1.0]:\n",
    "        print()\n",
    "        print('----- temperature:', temperature)\n",
    "\n",
    "        textX = text[start_index: start_index + W]\n",
    "        print('----- Generating with seed: \"' + textX + '\"')\n",
    "        inp = char_tensor(textX)\n",
    "        inp = torch.unsqueeze(inp, 0)  # turn a [100] into [1, 100]\n",
    "        \n",
    "        for i in range(pred_len):\n",
    "            # forward pass\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            output, hidden = model_test(inp, hidden)\n",
    "            \n",
    "            # logsoftmax returns negative numbers. we undo that to sample from the array\n",
    "            output_dist = output.view(-1).div(temperature).exp().cpu()\n",
    "            top_i = torch.multinomial(output_dist, 1)[0]\n",
    "            top_num = top_i.data[0]\n",
    "            pred_char = idx_to_chars[top_num]\n",
    "            \n",
    "            # adds the new char to the text and remove the first char\n",
    "            textX += pred_char\n",
    "            textX = textX[1:]\n",
    "            inp = char_tensor(textX)\n",
    "            \n",
    "            # print out the new char\n",
    "            sys.stdout.write(pred_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "        \n",
    "pred_text(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
