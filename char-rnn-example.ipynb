{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char-RNN in Pytorch: The Wisdom of Marx\n",
    "\n",
    "Let's try to implement to implement [Andrej's minmal char-RNN](https://gist.github.com/karpathy/d4dee566867f8291f086) to generate text in Pytorch! The difference is that we'll use LSTM layers instead of vanilla RNN, and we'll do it in batches with GPU. \n",
    "\n",
    "Compared to the [Intermediate RNN tutorials on Pytorch's website](http://pytorch.org/tutorials/), the main difference is that this tutorial will take more advantage of the GPU, by taking in multiple words/characters at a time, and in batches.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "First we acquire the MS word version of [Das Kapital Volume 1](https://www.marxists.org/archive/marx/works/1867-c1/) and turned it into a text format that we can load. Let's see a snippet of it as well.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1: Commodities\\nSection 1: The Two Factors of a Commodity:\\nUse-Value and Value\\n(The Substance of Value and the Magnitude of Value)\\nThe wealth of those societies in which the capitalist mode of production prevails, presents itself as \\x93an immense accumulation of commodities,\\x941 its unit being a single commodity. Our investigation must therefore begin with the analysis of a commodity. \\nA commodity is, in the first place, an object outside us, a thing that by its properties satisfies human wants of some sort or another. The nature of such wants, whether, for instance, they spring from the stomach or from fancy, makes no difference.2 Neither are we here concerned to know how the object satisfies these wants, whether directly as means of subsistence, or indirectly as means of production. \\nEvery useful thing, as iron, paper, &c., may be looked at from the two points of view of quality and quantity. It is an assemblage of many properties, and may therefore be of use in various ways. To d'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text = open('capital-vol1.txt', encoding='latin-1', mode='r').read()\n",
    "raw_text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! Let's print out all the characters in the text to see what we will be getting rid of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus has 1468303 letters altogether\n",
      "corpus has 108 unique characters: ['\\n', ' ', '!', '\"', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}', '\\x91', '\\x92', '\\x93', '\\x94', '\\x96', '\\xa0', '£', '°', '¼', '½', '¾', '×', 'à', 'â', 'æ', 'è', 'é', 'ê', 'î', 'ï', 'ô', 'û', 'ü']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(raw_text))\n",
    "print('corpus has ' + str(len(raw_text)) + ' letters altogether')\n",
    "print ('corpus has ' + str(len(chars)) + ' unique characters:', chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll clean up the text so that our output is limited to lower cased english characters plus simple punctuations. The cleaning function is taken from [stackoverflow](#https://stackoverflow.com/questions/517923/what-is-the-best-way-to-remove-accents-in-a-python-unicode-string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus has 1427555 letters altogether\n",
      "corpus has 32 unique characters after cleaning: {'d', 'h', 't', 'u', 'y', ';', '-', '.', 'c', 'i', \"'\", 'q', 'z', 'k', 'm', 'o', 'p', 's', 'e', 'f', ' ', 'a', 'r', ',', 'w', 'v', 'g', 'j', 'n', 'l', 'b', 'x'}\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_lowercase + \" .,;'-\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "text = unicodeToAscii(raw_text)\n",
    "text_length = len(text)\n",
    "print('corpus has ' + str(text_length) + ' letters altogether')\n",
    "print ('corpus has ' + str(len(set(text))) + ' unique characters after cleaning:', set(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, though we can probably expect certain words with non-english characters to be misspelled. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting text to vectors\n",
    "\n",
    "To create out dataset, we begin by creating a list of inputs and outputs. Our input should be a string of text, say 100 characters long (we will call this sequence length), and the output should be the next character following it. We can decide how many samples we want and how redundant they should be with the step size. The step size indicates how many characters we move ahead for every sample we create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample size =  6 ['hello', 'ello ', 'llo w', 'lo wo', 'o wor', ' worl']\n",
      "sample size =  6 [' ', 'w', 'o', 'r', 'l', 'd']\n"
     ]
    }
   ],
   "source": [
    "def textToWin(text, seq_len, step_size):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for i in range(0, len(text) - seq_len, step_size):\n",
    "        window = text[i:seq_len+i]\n",
    "        inputs.append(window)\n",
    "    outputs = [i for i in text[seq_len::step_size]]\n",
    "    return inputs, outputs\n",
    "\n",
    "inptest, outtest = textToWin('hello world', 5, 1)\n",
    "print(\"sample size = \", len(inptest), inptest)\n",
    "print(\"sample size = \", len(outtest), outtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we incorporate the previous function and turn the outputted list of strings into tensors of indices (aka vectors), so that our model can process them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup idx and char mapping \n",
    "chars_to_idx = dict((c, i) for i, c in enumerate(all_letters))\n",
    "idx_to_chars = dict((i, c) for i, c in enumerate(all_letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  7   4  11  11  14\n",
      " 11  11  14  26  22\n",
      " 14  26  22  14  17\n",
      "[torch.LongTensor of size 3x5]\n",
      " \n",
      " 26\n",
      " 14\n",
      " 11\n",
      "[torch.LongTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def textToTensor(text, seq_len, step_size):\n",
    "    inputs, outputs = textToWin(text, seq_len, step_size)\n",
    "    X = torch.zeros(len(inputs), win_size).long()\n",
    "    y = torch.zeros(len(inputs)).long()\n",
    "    for i, seq in enumerate(inputs):\n",
    "        for t, char in enumerate(seq):\n",
    "            X[i, t] = chars_to_idx[seq[t]]\n",
    "        y[i] = chars_to_idx[outputs[i]]\n",
    "    # outputs X, y - (sample_size, seq_len), (sample_size) with value 0 < c < n_letters\n",
    "    return X, y\n",
    "\n",
    "test_text = \"hello world\"\n",
    "testX, testy = textToTensor(test_text, 5, 2)\n",
    "print(testX, testy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dataset with data.TensorDataset and use data.DataLoader to pack our X and y into mini-batches. We enumerate through the packed train_loader to get X of (batch_size, seq_len) shape and y of (batch_size). Realize that taking the total sample size of the unpacked X and dividing that by the batch size gives us about the same number as the length of the packed X, y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "929.333984375\n",
      "929\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data\n",
    "N = 512\n",
    "step_size=3\n",
    "\n",
    "X, y = textToTensor(text, W, step_size)\n",
    "dataset = torch.utils.data.TensorDataset(X, y)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=N, num_workers=1, pin_memory=True, drop_last=True)\n",
    "\n",
    "print (len(X)/N)\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time\n",
    "\n",
    "# we will need to specifiy the variables as cuda to utilize GPU\n",
    "cudafloat = torch.cuda.FloatTensor \n",
    "cudalong = torch.cuda.LongTensor\n",
    "\n",
    "'''\n",
    "W, H, N = seq_length, hidden_size, batch_size\n",
    "'''\n",
    "\n",
    "class LSTMText(nn.Module):\n",
    "    def __init__(self, W, H, n_layers, N, dropout, num_embed=n_letters):\n",
    "        super(LSTMText, self).__init__()\n",
    "        self.num_embed = num_embed\n",
    "        self.embed_dim = H\n",
    "        self.H = H\n",
    "        self.n_layers = n_layers\n",
    "        self.W = W\n",
    "        self.out_dim = num_embed\n",
    "        self.N = N\n",
    "        \n",
    "        self.encoder = nn.Embedding(num_embed, self.embed_dim)\n",
    "        self.lstm = nn.LSTM(self.embed_dim, H, n_layers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(H, self.out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.logsoftmax = nn.LogSoftmax()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        h0 = Variable(torch.zeros(n_layers, N, H).type(cudafloat))\n",
    "        c0 = Variable(torch.zeros(n_layers, N, H).type(cudafloat))\n",
    "        return h0, c0\n",
    "    \n",
    "    def forward(self, inputs, hidden):                  \n",
    "        embed = self.encoder(inputs)     # (N, W) => (N, W, embed_dim) \n",
    "        embed = embed.view(N, W, H)      # maintains 3D when N=1\n",
    "        embed = embed.permute(1, 0, 2)   # (N, W, embed_dim) => (W, N, embed_dim)\n",
    "        output, hidden = self.lstm(embed, hidden)   # (W, N, embed_dim) => (W, N, H)\n",
    "        output = output[W-1, :, :]        # select  last vector of W (last character)\n",
    "        decoded = self.decoder(output)              # (N, H) => (N, out_dim)\n",
    "        pred = self.logsoftmax(decoded)\n",
    "        \n",
    "        return pred, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function disconnect the hidden cells from its history. We need this because we do not ned to backpropagate through the entire history of updates to the hidden cells. \n",
    "[source](https://discuss.pytorch.org/t/help-clarifying-repackage-hidden-in-word-language-model/226/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    \"\"\"source: https://github.com/pytorch/examples/tree/master/word_language_model\"\"\"\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we instantiate the model and send it to the GPU with model.cuda(), and specify our update method and loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "W = 100\n",
    "H = 500\n",
    "n_layers = 2\n",
    "dropout = 0.3\n",
    "\n",
    "model = LSTMText(W=W, H=H, n_layers=n_layers, N=N, dropout=dropout)\n",
    "model.cuda()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss(size_average=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 1.468 tijme: 177.984\n",
      "[2] loss: 1.216 tijme: 175.426\n",
      "[3] loss: 1.143 tijme: 177.457\n",
      "[4] loss: 1.095 tijme: 177.383\n",
      "[5] loss: 1.059 tijme: 177.394\n",
      "[6] loss: 1.030 tijme: 177.512\n",
      "[7] loss: 1.006 tijme: 177.998\n",
      "[8] loss: 0.983 tijme: 178.302\n",
      "[9] loss: 0.964 tijme: 178.291\n",
      "[10] loss: 0.946 tijme: 178.349\n",
      "[11] loss: 0.929 tijme: 178.312\n",
      "[12] loss: 0.915 tijme: 178.395\n",
      "[13] loss: 0.902 tijme: 178.246\n",
      "[14] loss: 0.888 tijme: 178.348\n",
      "[15] loss: 0.877 tijme: 178.272\n",
      "[16] loss: 0.866 tijme: 178.354\n",
      "[17] loss: 0.856 tijme: 178.352\n",
      "[18] loss: 0.847 tijme: 178.357\n",
      "[19] loss: 0.840 tijme: 178.410\n",
      "[20] loss: 0.831 tijme: 178.370\n",
      "[21] loss: 0.823 tijme: 178.362\n",
      "[22] loss: 0.818 tijme: 178.340\n",
      "[23] loss: 0.812 tijme: 178.254\n",
      "[24] loss: 0.806 tijme: 178.376\n",
      "[25] loss: 0.802 tijme: 178.299\n",
      "[26] loss: 0.797 tijme: 178.419\n",
      "[27] loss: 0.792 tijme: 178.348\n",
      "[28] loss: 0.790 tijme: 178.353\n",
      "[29] loss: 0.786 tijme: 178.442\n",
      "[30] loss: 0.782 tijme: 178.341\n",
      "[31] loss: 0.779 tijme: 178.438\n",
      "[32] loss: 0.776 tijme: 178.423\n",
      "[33] loss: 0.774 tijme: 178.395\n",
      "[34] loss: 0.771 tijme: 178.440\n",
      "[35] loss: 0.771 tijme: 178.517\n",
      "[36] loss: 0.770 tijme: 178.501\n",
      "[37] loss: 0.766 tijme: 178.553\n",
      "[38] loss: 0.764 tijme: 178.444\n",
      "[39] loss: 0.763 tijme: 178.480\n",
      "[40] loss: 0.763 tijme: 178.411\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    hidden = model.init_hidden()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # send input to GPU and wrap in torch Variable\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "             \n",
    "        # forward, backward, optimize\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data[0]/len(train_loader)\n",
    "        \n",
    "    print('[%d] loss: %.3f tijme: %.3f' % (epoch + 1, running_loss, time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'pytorch_test_weight6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating text\n",
    "We create a new model with a batch_size of 1 to take in one sequence of text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "N = 1\n",
    "\n",
    "model_test = LSTMText(W=W, H=H, n_layers=n_layers, N=N, dropout=dropout)\n",
    "model_test.cuda()\n",
    "model_test.eval()\n",
    "model_test.load_state_dict(torch.load('pytorch_test_weight6'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "  7\n",
      "  4\n",
      " 11\n",
      " 11\n",
      " 14\n",
      "[torch.cuda.LongTensor of size 5 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long().cuda()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = chars_to_idx[string[c]]\n",
    "    return Variable(tensor)\n",
    "\n",
    "a = char_tensor('hello')\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \" that the human carpenter employs on wood; the instrument that, on the ondon wharves, cuts the venee\"\n",
      "d of the labourer from the beds, that indirectly upon contract, therefore, the labourer for the other cases, the productive power of labour in the expropriation of the labourer than a capitalist and moral employed in the first transformation of the labourer himself. he total labour power as a whole, in the following as the same time, still more rapidly than the agricultural population, therefore, \n",
      "\n",
      "----- temperature: 0.8\n",
      "----- Generating with seed: \" that the human carpenter employs on wood; the instrument that, on the ondon wharves, cuts the venee\"\n",
      "d of the districts of his policical economists, where will is the mere system of capitalistic production, becomes a question of labour therefore or tools, the condition of the old of the soil, things in the first districts of  centimes among which these two opinions, the descriptional capital, or the value of the antithetical producer, who follows to one another, the labourer himself to use evil o\n",
      "\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \" that the human carpenter employs on wood; the instrument that, on the ondon wharves, cuts the venee\"\n",
      "d with a mind-boxion of manyers , was shops, and the payart of the nited ingdom, as a buildings and aardan for males reciproduced into means of production, and of the society, energhised into  was lessened flows that depression of nglish capitals and falled period and fell. he expropriation of the capital of relation were determined by the mode of production. nd one biddle capitalists existed the \n"
     ]
    }
   ],
   "source": [
    "def pred_text(pred_len):\n",
    "    start_index = random.randint(0, len(text) - W - 1)\n",
    "    hidden = model_test.init_hidden()\n",
    "    \n",
    "    for temperature in [0.5, 0.8, 1.0]:\n",
    "        print()\n",
    "        print('----- temperature:', temperature)\n",
    "\n",
    "        textX = text[start_index: start_index + W]\n",
    "        print('----- Generating with seed: \"' + textX + '\"')\n",
    "        inp = char_tensor(textX)\n",
    "        inp = torch.unsqueeze(inp, 0)  # turn a [100] into [1, 100]\n",
    "        \n",
    "        for i in range(pred_len):\n",
    "            # forward pass\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            output, hidden = model_test(inp, hidden)\n",
    "            \n",
    "            # logsoftmax returns negative numbers. we undo that to sample from the array\n",
    "            output_dist = output.view(-1).div(temperature).exp().cpu()\n",
    "            top_i = torch.multinomial(output_dist, 1)[0]\n",
    "            top_num = top_i.data[0]\n",
    "            pred_char = idx_to_chars[top_num]\n",
    "            \n",
    "            # adds the new char to the text and remove the first char\n",
    "            textX += pred_char\n",
    "            textX = textX[1:]\n",
    "            inp = char_tensor(textX)\n",
    "            \n",
    "            # print out the new char\n",
    "            sys.stdout.write(pred_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "        \n",
    "pred_text(400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
